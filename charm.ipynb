{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import add\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def scaling(x, scale):\n",
    "    return x * scale\n",
    "\n",
    "\n",
    "def conv2d_bn(x,\n",
    "              filters,\n",
    "              kernel_size,\n",
    "              strides=1,\n",
    "              padding='same',\n",
    "              activation='relu',\n",
    "              use_bias=False,\n",
    "              name=None):\n",
    "    x = Conv2D(filters,\n",
    "               kernel_size,\n",
    "               strides=strides,\n",
    "               padding=padding,\n",
    "               use_bias=use_bias,\n",
    "               name=name)(x)\n",
    "    if not use_bias:\n",
    "        bn_axis = 1 if K.image_data_format() == 'channels_first' else 3\n",
    "        bn_name = _generate_layer_name('BatchNorm', prefix=name)\n",
    "        x = BatchNormalization(axis=bn_axis, momentum=0.995, epsilon=0.001,\n",
    "                               scale=False, name=bn_name)(x)\n",
    "    if activation is not None:\n",
    "        ac_name = _generate_layer_name('Activation', prefix=name)\n",
    "        x = Activation(activation, name=ac_name)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def _generate_layer_name(name, branch_idx=None, prefix=None):\n",
    "    if prefix is None:\n",
    "        return None\n",
    "    if branch_idx is None:\n",
    "        return '_'.join((prefix, name))\n",
    "    return '_'.join((prefix, 'Branch', str(branch_idx), name))\n",
    "\n",
    "\n",
    "def _inception_resnet_block(x, scale, block_type, block_idx, activation='relu'):\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n",
    "    if block_idx is None:\n",
    "        prefix = None\n",
    "    else:\n",
    "        prefix = '_'.join((block_type, str(block_idx)))\n",
    "    name_fmt = partial(_generate_layer_name, prefix=prefix)\n",
    "\n",
    "    if block_type == 'Block35':\n",
    "        branch_0 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_1x1', 0))\n",
    "        branch_1 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 32, 3, name=name_fmt('Conv2d_0b_3x3', 1))\n",
    "        branch_2 = conv2d_bn(x, 32, 1, name=name_fmt('Conv2d_0a_1x1', 2))\n",
    "        branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0b_3x3', 2))\n",
    "        branch_2 = conv2d_bn(branch_2, 32, 3, name=name_fmt('Conv2d_0c_3x3', 2))\n",
    "        branches = [branch_0, branch_1, branch_2]\n",
    "    elif block_type == 'Block17':\n",
    "        branch_0 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_1x1', 0))\n",
    "        branch_1 = conv2d_bn(x, 128, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 128, [1, 7], name=name_fmt('Conv2d_0b_1x7', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 128, [7, 1], name=name_fmt('Conv2d_0c_7x1', 1))\n",
    "        branches = [branch_0, branch_1]\n",
    "    elif block_type == 'Block8':\n",
    "        branch_0 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_1x1', 0))\n",
    "        branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 192, [1, 3], name=name_fmt('Conv2d_0b_1x3', 1))\n",
    "        branch_1 = conv2d_bn(branch_1, 192, [3, 1], name=name_fmt('Conv2d_0c_3x1', 1))\n",
    "        branches = [branch_0, branch_1]\n",
    "    else:\n",
    "        raise ValueError('Unknown Inception-ResNet block type. '\n",
    "                         'Expects \"Block35\", \"Block17\" or \"Block8\", '\n",
    "                         'but got: ' + str(block_type))\n",
    "\n",
    "    mixed = Concatenate(axis=channel_axis, name=name_fmt('Concatenate'))(branches)\n",
    "    up = conv2d_bn(mixed,\n",
    "                   K.int_shape(x)[channel_axis],\n",
    "                   1,\n",
    "                   activation=None,\n",
    "                   use_bias=True,\n",
    "                   name=name_fmt('Conv2d_1x1'))\n",
    "    up = Lambda(scaling,\n",
    "                output_shape=K.int_shape(up)[1:],\n",
    "                arguments={'scale': scale})(up)\n",
    "    x = add([x, up])\n",
    "    if activation is not None:\n",
    "        x = Activation(activation, name=name_fmt('Activation'))(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def InceptionResNetV1(input_shape=(160, 160, 3),\n",
    "                      classes=128,\n",
    "                      dropout_keep_prob=0.8,\n",
    "                      weights_path=None):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = conv2d_bn(inputs, 32, 3, strides=2, padding='valid', name='Conv2d_1a_3x3')\n",
    "    x = conv2d_bn(x, 32, 3, padding='valid', name='Conv2d_2a_3x3')\n",
    "    x = conv2d_bn(x, 64, 3, name='Conv2d_2b_3x3')\n",
    "    x = MaxPooling2D(3, strides=2, name='MaxPool_3a_3x3')(x)\n",
    "    x = conv2d_bn(x, 80, 1, padding='valid', name='Conv2d_3b_1x1')\n",
    "    x = conv2d_bn(x, 192, 3, padding='valid', name='Conv2d_4a_3x3')\n",
    "    x = conv2d_bn(x, 256, 3, strides=2, padding='valid', name='Conv2d_4b_3x3')\n",
    "\n",
    "    # 5x Block35 (Inception-ResNet-A block):\n",
    "    for block_idx in range(1, 6):\n",
    "        x = _inception_resnet_block(x,\n",
    "                                    scale=0.17,\n",
    "                                    block_type='Block35',\n",
    "                                    block_idx=block_idx)\n",
    "      # Mixed 6a (Reduction-A block):\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else 3\n",
    "    name_fmt = partial(_generate_layer_name, prefix='Mixed_6a')\n",
    "    branch_0 = conv2d_bn(x,\n",
    "                         384,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 0))\n",
    "    branch_1 = conv2d_bn(x, 192, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "    branch_1 = conv2d_bn(branch_1, 192, 3, name=name_fmt('Conv2d_0b_3x3', 1))\n",
    "    branch_1 = conv2d_bn(branch_1,\n",
    "                         256,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 1))\n",
    "    branch_pool = MaxPooling2D(3,\n",
    "                               strides=2,\n",
    "                               padding='valid',\n",
    "                               name=name_fmt('MaxPool_1a_3x3', 2))(x)\n",
    "    branches = [branch_0, branch_1, branch_pool]\n",
    "    x = Concatenate(axis=channel_axis, name='Mixed_6a')(branches)\n",
    "\n",
    "    # 10x Block17 (Inception-ResNet-B block):\n",
    "    for block_idx in range(1, 11):\n",
    "        x = _inception_resnet_block(x,\n",
    "                                    scale=0.1,\n",
    "                                    block_type='Block17',\n",
    "                                    block_idx=block_idx)\n",
    "\n",
    "    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n",
    "    name_fmt = partial(_generate_layer_name, prefix='Mixed_7a')\n",
    "    branch_0 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 0))\n",
    "    branch_0 = conv2d_bn(branch_0,\n",
    "                         384,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 0))\n",
    "    branch_1 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 1))\n",
    "    branch_1 = conv2d_bn(branch_1,\n",
    "                         256,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 1))\n",
    "    branch_2 = conv2d_bn(x, 256, 1, name=name_fmt('Conv2d_0a_1x1', 2))\n",
    "    branch_2 = conv2d_bn(branch_2, 256, 3, name=name_fmt('Conv2d_0b_3x3', 2))\n",
    "    branch_2 = conv2d_bn(branch_2,\n",
    "                         256,\n",
    "                         3,\n",
    "                         strides=2,\n",
    "                         padding='valid',\n",
    "                         name=name_fmt('Conv2d_1a_3x3', 2))\n",
    "    branch_pool = MaxPooling2D(3,\n",
    "                               strides=2,\n",
    "                               padding='valid',\n",
    "                               name=name_fmt('MaxPool_1a_3x3', 3))(x)\n",
    "    branches = [branch_0, branch_1, branch_2, branch_pool]\n",
    "    x = Concatenate(axis=channel_axis, name='Mixed_7a')(branches)\n",
    "\n",
    "    # 5x Block8 (Inception-ResNet-C block):\n",
    "    for block_idx in range(1, 6):\n",
    "        x = _inception_resnet_block(x,\n",
    "                                    scale=0.2,\n",
    "                                    block_type='Block8',\n",
    "                                    block_idx=block_idx)\n",
    "    x = _inception_resnet_block(x,\n",
    "                                scale=1.,\n",
    "                                activation=None,\n",
    "                                block_type='Block8',\n",
    "                                block_idx=6)\n",
    "\n",
    "    # Classification block\n",
    "    x = GlobalAveragePooling2D(name='AvgPool')(x)\n",
    "    x = Dropout(1.0 - dropout_keep_prob, name='Dropout')(x)\n",
    "    # Bottleneck\n",
    "    x = Dense(classes, use_bias=False, name='Bottleneck')(x)\n",
    "    bn_name = _generate_layer_name('BatchNorm', prefix='Bottleneck')\n",
    "    x = BatchNormalization(momentum=0.995, epsilon=0.001, scale=False,\n",
    "                           name=bn_name)(x)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs, x, name='inception_resnet_v1')\n",
    "    if weights_path is not None:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model.\n",
    "embeddings_generator = InceptionResNetV1(\n",
    "        input_shape=(None, None, 3),\n",
    "        classes=128,\n",
    "    )\n",
    "# Loading the prebuilt weights.\n",
    "embeddings_generator.load_weights('facenet_keras_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder,Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import FileLink\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_and_labels(dir_path,model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dir_path : base directory path that contains the folders of each actor.\n",
    "    model : facenet model that is loaded with trained weights. (Transfer learning)\n",
    "    \n",
    "    Functionality\n",
    "    -------------\n",
    "    1. Function reads all the images in the base directory by traversing recursively across all folders.\n",
    "    2. All the imagees are standardized and the 128 face embeddings will be generated for all the images.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "    face_pixels : list of numpy arrays where each numpy array represents an image (face).\n",
    "    face_embeddings : list of embeddings for all the images read.\n",
    "    labels : list of labels where each label represents the name of the actor\n",
    "    \"\"\"\n",
    "    face_pixels,face_embeddings,labels=[],[],[]\n",
    "    for actor_dir in tqdm(os.listdir(dir_path)):\n",
    "        for actor_image in os.listdir(dir_path+actor_dir):\n",
    "            # reading the image\n",
    "            img=cv2.imread(dir_path+actor_dir+'/'+actor_image)\n",
    "            \n",
    "            # converting the image to RGB format\n",
    "            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resizing the image to the size required by Facenet\n",
    "            img=cv2.resize(img,(160,160))\n",
    "            img = img.astype('float32')\n",
    "            \n",
    "            # Normalizing the images\n",
    "            mean, std = img.mean(), img.std()\n",
    "            img = (img - mean) / std\n",
    "            face_pixels.append(img)\n",
    "            labels.append(actor_dir)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            \n",
    "            # Getting the face embeddings from the model.\n",
    "            face_embeddings.append(model.predict(img))\n",
    "    return face_pixels,face_embeddings,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir='./de/'\n",
    "face_pixels,face_embeddings,labels=load_images_and_labels(base_dir,embeddings_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the lists into numpy arrays\n",
    "face_embeddings=np.array(face_embeddings)\n",
    "face_pixels=np.array(face_pixels)\n",
    "labels=np.array(labels)\n",
    "face_embeddings=face_embeddings.reshape(face_embeddings.shape[0],face_embeddings.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving these files to overcome processing time of the dataset\n",
    "np.savez_compressed('datas/data.npz',a=face_embeddings,b=face_pixels,c=labels)\n",
    "FileLink(r'data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading back the generated embeddings, face images and labels\n",
    "data=np.load('datas/data.npz')\n",
    "face_embeddings,face_pixels,labels=data['a'],data['b'],data['c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING EMBEDDING AUTHENTICITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the authenticity of the generated emebeddings\n",
    "temp=pd.DataFrame(face_embeddings)\n",
    "print(\"Total embeddings generated: \",len(temp))\n",
    "print(\"Total embeddings after dropping duplicates: \",len(temp.drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a label encoder of names of actors\n",
    "label_encoder=LabelEncoder()\n",
    "encoded_labels=label_encoder.fit_transform(labels)\n",
    "\n",
    "# storing the label encoder object for using in deplyment environment\n",
    "joblib.dump(label_encoder, 'datas/label_encoder.sav')\n",
    "\n",
    "# Downloading the picked label encoder file\n",
    "FileLink(r'label_encoder.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(face_embeddings,encoded_labels,test_size=0.3,stratify=encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of Xtrain : {x_train.shape}\")\n",
    "print(f\"Shape of Ytrain : {y_train.shape}\")\n",
    "print(f\"Shape of Xtest : {x_test.shape}\")\n",
    "print(f\"Shape of Ytest : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(probability=True)\n",
    "model.fit(x_train,y_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_train = model.predict(x_train)\n",
    "yhat_test = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating accuracy score on train set\n",
    "score_train = accuracy_score(y_train, yhat_train)\n",
    "\n",
    "# calculating accuracy score on test set\n",
    "score_test = accuracy_score(y_test, yhat_test)\n",
    "\n",
    "print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out probabilites of first test sample for every target class\n",
    "probs_svc = model.decision_function([x_test[0]])\n",
    "probs_svc = (probs_svc - probs_svc.min()) / (probs_svc.max() - probs_svc.min())\n",
    "probs_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yhat_test,y_test,target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(yhat_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, 'datas/svm_classifier.sav')\n",
    "FileLink(r'svm_classifier.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index=np.random.choice(len(labels))\n",
    "predicted_label=label_encoder.inverse_transform(model.predict([face_embeddings[random_index]]))[0]\n",
    "actual_label=labels[random_index]\n",
    "plt.title(f\"Actual Label: {actual_label} \")\n",
    "plt.xlabel(f\"Predicted Label: {predicted_label}\")\n",
    "plt.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "plt.imshow(face_pixels[random_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving one embedding for each face\n",
    "mapping_dict = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "known_face_embeddings=[]\n",
    "for key in mapping_dict:\n",
    "    match_indices=np.where(labels==key)\n",
    "    first_match_index=match_indices[0][0]\n",
    "    known_face_embeddings.append(face_embeddings[first_match_index])\n",
    "known_face_embeddings=np.array(known_face_embeddings)\n",
    "np.savez_compressed('datas/data.npz',a=known_face_embeddings)\n",
    "FileLink(r'data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the pre-trained SVM model and label encoder\n",
    "model = joblib.load('datas/svm_classifier.sav')\n",
    "label_encoder = joblib.load('datas/label_encoder.sav')\n",
    "\n",
    "# Load the face embeddings of known faces\n",
    "known_face_data = np.load('datas/data.npz')\n",
    "known_face_embeddings = known_face_data['a']\n",
    "\n",
    "# Load the face recognition model\n",
    "# Initializing the model.\n",
    "embeddings_generator = InceptionResNetV1(\n",
    "        input_shape=(None, None, 3),\n",
    "        classes=128,\n",
    "    )\n",
    "# Loading the prebuilt weights.\n",
    "embeddings_generator.load_weights('facenet_keras_weights.h5')\n",
    "def recognize_faces(frame, threshold=0.7):\n",
    "    # Detect faces in the frame\n",
    "    faces = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml').detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Preprocess the face for recognition\n",
    "        face = cv2.resize(face, (160, 160))\n",
    "        face = (face.astype('float32') - 127.5) / 128.0  # Normalize to the range [-1, 1]\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "        \n",
    "        # Generate face embeddings using the pre-trained model\n",
    "        face_embedding = embeddings_generator.predict(face).flatten()\n",
    "        \n",
    "        # Calculate cosine similarity between the face and known faces\n",
    "        similarities = [1 - cosine(face_embedding, known_face) for known_face in known_face_embeddings]\n",
    "        print(similarities)\n",
    "        # Find the maximum similarity\n",
    "        max_similarity = max(similarities)\n",
    "        \n",
    "        # Verify if the face is similar enough to any known face\n",
    "        if max_similarity >= threshold:\n",
    "            # Get the label corresponding to the most similar known face\n",
    "            predicted_label = label_encoder.classes_[np.argmax(similarities)]\n",
    "            \n",
    "            # Draw the bounding box and label on the frame\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Perform face recognition on the frame\n",
    "    frame = recognize_faces(frame)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Recognition', frame)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the OpenCV window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model.\n",
    "embeddings_generator = InceptionResNetV1(\n",
    "        input_shape=(None, None, 3),\n",
    "        classes=128,\n",
    "    )\n",
    "# Loading the prebuilt weights.\n",
    "embeddings_generator.load_weights('facenet_keras_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture to JSON file\n",
    "model_json = embeddings_generator.to_json()\n",
    "with open(\"datas/model_architecture.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model weights to an HDF5 file\n",
    "embeddings_generator.save_weights(\"datas/model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da/label_encoder.sav']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(label_encoder, 'datas/label_encoder.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da/svm_classifier.sav']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'datas/svm_classifier.sav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aarvm\\miniconda3\\envs\\py311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Assuming 'embeddings_generator' is your Keras model\n",
    "# Compile your model before saving it\n",
    "embeddings_generator.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the model to an HDF5 file\n",
    "model_path = 'datas/your_model.h5'\n",
    "embeddings_generator.save(model_path)\n",
    "\n",
    "# Load the model on a new machine\n",
    "loaded_model = load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "POS_PATH_R = os.path.join('de', 'p1')\n",
    "POS_PATH_K= os.path.join('de', 'p2')\n",
    "POS_PATH_S = os.path.join('de' , 'p3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_counter = 1\n",
    "k_counter = 1\n",
    "# Establish a connection to the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "   \n",
    "    # Cut down frame to 250x250px\n",
    "    frame = frame[:,:, :]\n",
    "    \n",
    "    # Collect anchors \n",
    "    if cv2.waitKey(1) & 0XFF == ord('a'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH_S, 'S{:03d}.jpg'.format(r_counter))\n",
    "        # Increment the counter for R\n",
    "        r_counter += 1\n",
    "        # Write out positive image for R\n",
    "        cv2.imwrite(imgname, frame)\n",
    "    \n",
    "    # Collect positives\n",
    "    if cv2.waitKey(1) & 0XFF == ord('p'):\n",
    "        # Create the unique file path \n",
    "        imgname = os.path.join(POS_PATH_K, 'K{:03d}.jpg'.format(k_counter))\n",
    "        # Increment the counter for K\n",
    "        k_counter += 1\n",
    "        # Write out positive image for K\n",
    "        cv2.imwrite(imgname, frame)\n",
    "\n",
    "        # Show image back to screen\n",
    "    cv2.imshow('Image Collection', frame)\n",
    "    \n",
    "    # Breaking gracefully\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "# Release the webcam\n",
    "cap.release()\n",
    "# Close the image show frame\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
